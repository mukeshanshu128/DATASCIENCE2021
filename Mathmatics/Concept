CONCEPT1 : odds vs Logodds

Probability to Odds to Log of Odds
  Probability is the probability an event happens. For example, there might be an 80% chance of rain today.
  Odds (more technically the odds of success) is defined as probability of success/probability of failure. So the odds of a success (80% chance of rain) has an accompanying odds of failure (20% chance it doesn’t rain); as an equation (the “odds ratio“), that’s .8/.2 = 4.
  Log odds is the logarithm of the odds. Ln(4) = 1.38629436 ≅ 1.386.

 The odds ratio is the probability of success/probability of failure
 
 As an equation, that’s P(A)/P(-A), where P(A) is the probability of A, and P(-A) the probability of ‘not A’ (i.e. the complement of A).
 
 Taking the logarithm of the odds ratio gives us the log odds of A, which can be written as

 log(A) = log(P(A)/P(-A))   or  log [p/(1-p)]
 
  ->ODDS VS PROBABILITY
  
  logit function=log(P(A)/P(-A))
  SIGMOID FUNCTION- inverse of Logit function->   Y = 1 /1+e -z
  
  Sigmoid Function acts as an activation function in machine learning which is used to add non-linearity in a machine learning model, 
  in simple words it decides which value to pass as output and what not to pass, there are mainly 7 types of Activation 
  Functions which are used in machine learning and deep learning
